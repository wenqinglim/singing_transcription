{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wenqinglim/workdir/qmul/singing_transcription/.venv/lib/python3.11/site-packages/starlette/formparsers.py:12: FutureWarning: Please use `import python_multipart` instead.\n",
      "  import multipart\n"
     ]
    }
   ],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "import librosa\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.pipelines import CONVTASNET_BASE_LIBRI2MIX\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "from transformers import pipeline\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.crepe_model import CREPEModel\n",
    "\n",
    "# use GPU if available, otherwise, use cpu\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_audio_path = \"../audio/test_data/leon_7_03_jmzen_5_03.wav\"\n",
    "audio, sample_rate = torchaudio.load(f\"{mixed_audio_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized CONVTASNET_BASE_LIBRI2MIX model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wenqinglim/workdir/qmul/singing_transcription/.venv/lib/python3.11/site-packages/torchaudio/pipelines/_source_separation_pipeline.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path)\n"
     ]
    }
   ],
   "source": [
    "model = CONVTASNET_BASE_LIBRI2MIX.get_model()\n",
    "model = model.to(device)\n",
    "print(f\"Initialized CONVTASNET_BASE_LIBRI2MIX model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pred = model(audio.reshape(1, 1, -1).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 54785])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save split audio samples\n",
    "torchaudio.save(f'../audio/test_data/split_full_1.wav', F.normalize(pred[:1, 0]), 8000)\n",
    "torchaudio.save(f'../audio/test_data/split_full_2.wav', F.normalize(pred[:1, 1]), 8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_frames(vocals):\n",
    "    # make 1024-sample frames of the audio with hop length of 10 milliseconds\n",
    "    num_samples = len(vocals)\n",
    "    num_frames = int((num_samples - 1024) / 160) + 1\n",
    "    frames = vocals.unfold(step=160, size=1024, dimension=0)\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocals = pred[:1, 0]\n",
    "vocals = librosa.resample(vocals.cpu().numpy(), orig_sr=8000, target_sr=16000)\n",
    "frames = audio_to_frames(torch.tensor(vocals.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 109570)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([679, 1, 1024])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CREPEModel(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(1, 512, kernel_size=(512, 1), stride=(4, 1), padding=(254, 0), bias=False)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Dropout(p=0.25, inplace=False)\n",
       "    (5): Conv2d(512, 64, kernel_size=(64, 1), stride=(1, 1), padding=same, bias=False)\n",
       "    (6): ReLU()\n",
       "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Dropout(p=0.25, inplace=False)\n",
       "    (10): Conv2d(64, 64, kernel_size=(64, 1), stride=(1, 1), padding=same, bias=False)\n",
       "    (11): ReLU()\n",
       "    (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Dropout(p=0.25, inplace=False)\n",
       "    (15): Conv2d(64, 64, kernel_size=(64, 1), stride=(1, 1), padding=same, bias=False)\n",
       "    (16): ReLU()\n",
       "    (17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (18): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Dropout(p=0.25, inplace=False)\n",
       "    (20): Conv2d(64, 128, kernel_size=(64, 1), stride=(1, 1), padding=same, bias=False)\n",
       "    (21): ReLU()\n",
       "    (22): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (23): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Dropout(p=0.25, inplace=False)\n",
       "    (25): Conv2d(128, 256, kernel_size=(64, 1), stride=(1, 1), padding=same, bias=False)\n",
       "    (26): ReLU()\n",
       "    (27): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (28): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "    (29): Dropout(p=0.25, inplace=False)\n",
       "    (30): Flatten(start_dim=1, end_dim=-1)\n",
       "    (31): Linear(in_features=1024, out_features=411, bias=True)\n",
       "    (32): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = CREPEModel(mult).to(device)\n",
    "# model.load_state_dict(torch.load(f'best_crepe_{mult}.pkl'))\n",
    "# model.eval()\n",
    "\n",
    "model = CREPEModel.from_pretrained(\"omgitsqing/CREPE_MIR-1K_16\")\n",
    "model.eval()\n",
    "\n",
    "# pitches = model(vocals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = 0.3302\n",
    "train_std  = 0.6109"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torch.clip((frames - train_mean) / train_std, min=1e-8, max=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(TensorDataset(test_data, test_data), batch_size=20, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_range = list(librosa.midi_to_note([i/10 for i in range(360 , 770)], cents=True))\n",
    "note_range.append('silence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_notes = []\n",
    "for x, y in test_loader:\n",
    "    reshaped = torch.reshape(x, (x.shape[0],1024,1)).to(device)\n",
    "    pitches = model(reshaped)\n",
    "\n",
    "    # assign the note with the highest probability to each frame\n",
    "    pred_notes = [str(note_range[p.argmax()]) for p in pitches.cpu().detach().numpy()]\n",
    "    all_notes.extend(pred_notes)\n",
    "    # print(pitches.shape)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "679"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio(audio_file):\n",
    "    \"\"\"\n",
    "    Wrapper function for Gradio\n",
    "\n",
    "    Takes input audio and outputs two audio files, each containing one voice.\n",
    "    \"\"\"\n",
    "    if audio_file is None:\n",
    "        raise gr.Error(\"No audio file submitted!\")\n",
    "    sr, audio = audio_file\n",
    "\n",
    "    model = CONVTASNET_BASE_LIBRI2MIX.get_model()\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(torch.tensor(audio).float().reshape(1, 1, -1).to(device))\n",
    "\n",
    "    # output = pipe(audio_input, batch_size=BATCH_SIZE, generate_kwargs={\"task\": \"transcribe\"}, return_timestamps=True)\n",
    "    # vocal_1, vocal_2 = split_audio_pipe(audio)\n",
    "\n",
    "    torchaudio.save(f'split_1.wav', F.normalize(pred[0][:1, :]), 8000)\n",
    "    torchaudio.save(f'split_2.wav', F.normalize(pred[0][1:, :]), 8000)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return \"split_1.wav\", \"split_2.wav\"\n",
    "\n",
    "def audio_to_frames(vocals):\n",
    "    # make 1024-sample frames of the audio with hop length of 10 milliseconds\n",
    "    num_samples = len(vocals)\n",
    "    num_frames = int((num_samples - 1024) / 160) + 1\n",
    "    frames = vocals.unfold(step=160, size=1024, dimension=0)\n",
    "    return frames\n",
    "    \n",
    "def audio_to_pitch(audio_file):\n",
    "    \"\"\"\n",
    "    Wrapper function for Gradio\n",
    "\n",
    "    Takes input audio and outputs two audio files, each containing one voice.\n",
    "    \"\"\"\n",
    "    if audio_file is None:\n",
    "        raise gr.Error(\"No audio file submitted!\")\n",
    "    sr, audio = audio_file\n",
    "\n",
    "    vocals = librosa.resample(torch.tensor(audio).float().numpy(), orig_sr=sr, target_sr=16000)\n",
    "    frames = audio_to_frames(torch.tensor(vocals.T))\n",
    "\n",
    "    model = CREPEModel.from_pretrained(\"omgitsqing/CREPE_MIR-1K_32\")\n",
    "    model.eval()\n",
    "\n",
    "    train_mean = 0.3302\n",
    "    train_std  = 0.6109\n",
    "\n",
    "    test_data = torch.clip((frames - train_mean) / train_std, min=1e-8, max=None)\n",
    "\n",
    "    test_loader = DataLoader(TensorDataset(test_data, test_data), batch_size=20, shuffle=False)\n",
    "\n",
    "    note_range = list(librosa.midi_to_note([i/10 for i in range(360 , 770)], cents=True))\n",
    "    note_range.append('silence')\n",
    "\n",
    "    all_notes = []\n",
    "    for x, y in test_loader:\n",
    "        reshaped = torch.reshape(x, (x.shape[0],1024,1)).to(device)\n",
    "        pitches = model(reshaped)\n",
    "\n",
    "        # assign the note with the highest probability to each frame\n",
    "        pred_notes = [str(note_range[p.argmax()]) for p in pitches.cpu().detach().numpy()]\n",
    "        all_notes.extend(pred_notes)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return str(all_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"1500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = gr.Blocks()\n",
    "\n",
    "with demo:\n",
    "\n",
    "    # 1. Get users to record a duet\n",
    "    gr.Markdown(\n",
    "    \"\"\"\n",
    "    # Karaoke Chaos!\n",
    "    Sing your hearts out with a friend. We'll split the tracks and show you the notes you hit.\n",
    "    \"\"\")\n",
    "    moods = gr.State([])\n",
    "    audio_file = gr.Audio(type=\"numpy\")\n",
    "    gr.Examples(\n",
    "        examples=[\n",
    "            \"leon_7_03_jmzen_5_03.wav\",\n",
    "            \"leon_7_jmzen_5.wav\",\n",
    "        ],\n",
    "        inputs=audio_file,\n",
    "        # run_on_click=True,\n",
    "    )\n",
    "    b1 = gr.Button(\"Submit masterpiece\")\n",
    "\n",
    "    # 2. Let users listen to split tracks\n",
    "    gr.Markdown(\n",
    "    \"\"\"\n",
    "    ## Split Tracks:\n",
    "    Are your voices split as you expected? Listen to the tracks below.\n",
    "    \"\"\")\n",
    "    wav_path_1 = gr.Audio(interactive=False)\n",
    "    wav_path_2 = gr.Audio(interactive=False)\n",
    "\n",
    "    # 3. Let users re-upload pure pitches  \n",
    "    gr.Markdown(\n",
    "    \"\"\"\n",
    "    ## Transcribe Pitches:\n",
    "    Select `split_1`/`split_2` to transcribe your split vocals or upload a new audio file to transcribe the pitches.\n",
    "    \"\"\")\n",
    "    audio_file_to_transcribe = gr.Audio(type=\"numpy\")\n",
    "    gr.Examples(\n",
    "        examples=[\n",
    "            \"split_1.wav\",\n",
    "            \"split_2.wav\",\n",
    "        ],\n",
    "        inputs=audio_file_to_transcribe,\n",
    "    )\n",
    "    b2 = gr.Button(\"Transcribe Pitches\")\n",
    "\n",
    "    transcribed_pitch = gr.Textbox(\"Pitches\", type=\"text\")\n",
    "\n",
    "    b1.click(split_audio, inputs=audio_file, outputs=[wav_path_1, wav_path_2])\n",
    "    b2.click(audio_to_pitch, inputs=audio_file_to_transcribe, outputs=transcribed_pitch)\n",
    "\n",
    "    \n",
    "demo.launch(debug=True, height=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
